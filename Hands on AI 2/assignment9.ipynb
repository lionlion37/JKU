{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <p style=\"text-align: left;\">Name</p>               | Matr.Nr. | <p style=\"text-align: right;\">Date</p> |\n",
    "| --------------------------------------------------- | -------- | ------------------------------------- |\n",
    "| <p style=\"text-align: left\">Lion DUNGL</p> | 01553060 | 18.06.2020                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 9 (Assignment) -- Introduction to Reinforcement Learning -- Part II </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors</b>: Brandstetter, Schäfl <br>\n",
    "<b>Date</b>: 08-06-2020\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies \n",
    "to all code within this file.\n",
    "\n",
    "<b>Copyright statement</b>: <br>\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 0</h2>\n",
    "\n",
    "- Import the same modules as discussed in the lecture notebook.\n",
    "- Check if your model versions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import u9_utils as u9\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from typing import Any, Dict, Tuple\n",
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "\n",
    "# Set Seaborn plotting style.\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.7 (✓)\n",
      "Installed matplotlib version: 3.1.3 (✓)\n",
      "Installed Pandas version: 1.0.3 (✓)\n",
      "Installed Seaborn version: 0.10.1 (✓)\n",
      "Installed OpenAI Gym version: 0.17.2 (✓)\n"
     ]
    }
   ],
   "source": [
    "u9.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All exercises in this assignment are referring to the <i>FrozenLake-v0</i> environment of <a href=\"https://gym.openai.com\"><i>OpenAI Gym</i></a>. This environment is descibed according to its official <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">OpenAI Gym website</a> as follows:<br>\n",
    "<cite>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.</cite>\n",
    "\n",
    "\n",
    "There are <i>four</i> types of surfaces described in this environment:\n",
    "<ul>\n",
    "    <li><code>S</code> $\\rightarrow$ starting point (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>F</code> $\\rightarrow$ frozen surface (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>H</code> $\\rightarrow$ hole (<span style=\"color:rgb(255,0,0)\"><i>fall to your doom</i></span>)</li>\n",
    "    <li><code>G</code> $\\rightarrow$ goal (<span style=\"color:rgb(255,0,255)\"><i>frisbee location</i></span>)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "If not already done, more information on how to <i>install</i> and <i>import</i> the <code>gym</code> module is available in the lecture's notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">States and actions</h3>\n",
    "Experiment with the <i>FrozenLake-v0</i> environment as discussed during the lecture and explained in the accompanying notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_environment = FrozenLakeEnv()\n",
    "u9.set_seed(environment=lake_environment, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Current state ID: 0\n"
     ]
    }
   ],
   "source": [
    "lake_environment.render(mode=r'human')\n",
    "current_state_id = lake_environment.s\n",
    "print(f'\\nCurrent state ID: {current_state_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current position of the <i>disc retrieving</i> entity is displayed as a filled <span style=\"color:rgb(255,0,0)\"><i>red</i></span> rectangle.\n",
    "\n",
    "As we want to tackle this problem using our renowned <i>random search</i> approach, we have to analyse its applicability beforehand. Hence, the number of possible <i>actions</i> and <i>states</i> is of utter importance, as we don't want to get lost in the depth of combinatorial explosion.\n",
    "<ul>\n",
    "    <li>Query the amount of <i>actions</i> using the appropriate peoperty of the lake environment.</li>\n",
    "    <li>Query the amount of <i>states</i> using the appropriate property of the lake environment.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FrozenLake-v0 environment comprises <4> actions and <16> states.\n"
     ]
    }
   ],
   "source": [
    "num_actions = lake_environment.action_space.n\n",
    "num_states = lake_environment.observation_space.n\n",
    "print(f'The FrozenLake-v0 environment comprises <{num_actions}> actions and <{num_states}> states.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 1</h2>\n",
    "\n",
    "- Create a q_table for the frozen lake environment.\n",
    "- Apply $Q$-learning as it was done in the lecture to solve the environment.\n",
    "- Test the learned policy and animate one (or more) exemplary episode.\n",
    "- What do you observe? Does the agent learn anything useful? Discuss if something strange happens. Hint: print the q_table during training to better understand what is going on during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q-Table: (16, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of Q-Table: {q_table.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_q_learning(environment: lake_environment, alpha: float = 0.1):\n",
    "    \"\"\"\n",
    "    Solve lake_environment by applying Q learning.\n",
    "    \"\"\"\n",
    "    for i in range(1, 10001):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episode: {i}\")\n",
    "                print(q_table)\n",
    "\n",
    "    print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 5.84 s, sys: 1.3 s, total: 7.14 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "apply_q_learning(lake_environment, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 260.0\n",
      "Average dives per episode: 14.6\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_dives = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "captured_frames = [[] for _ in range(episodes)]\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # reset variables\n",
    "    epochs, dives = 0, 0\n",
    "    state = lake_environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        epochs += 1\n",
    "        \n",
    "        # take best step with regard to the Q-table\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = lake_environment.step(action)\n",
    "        \n",
    "        # reset done if done = True because dive was taken\n",
    "        if done and lake_environment.s != 15:\n",
    "            dives += 1\n",
    "            lake_environment.reset()\n",
    "            done = False\n",
    "            \n",
    "        captured_frames[episode].append({\n",
    "            r'frame': lake_environment.render(mode=r'ansi'),\n",
    "            r'state': state,\n",
    "            r'action': action,\n",
    "            r'reward': reward\n",
    "        })\n",
    "        \n",
    "        # safety switch; abort if number of epochs exceeds the number of steps it takes the random search method to reach the goal\n",
    "        if epochs == 260:\n",
    "            break\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_dives += dives\n",
    "    \n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average dives per episode: {total_dives / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step No.: 260\n",
      "State ID: 0\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[4], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step No.: 260\n",
      "State ID: 0\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[40], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "Step No.: 260\n",
      "State ID: 8\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[99], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "<b>The agent always takes the same action: action (0).</b> This can result in a step to the left, up or down (see Assignment 8). The agent only moves if a step down is made.\n",
    "\n",
    "The reason for this is that the Q-table never gets updated during the learning process (see above). Let's look at a few training steps:\n",
    "1. The initial state is state (0), the action taken is the one with the highest, estimated reward in that state. We've initialzed the Q-table only with zeros. Therefore, action (0) will be performed. (If all values in an array are the same, then np.argmax() will return the index 0)\n",
    "2. Action (0) in state (0) leads to no reward (the agent can only get a reward at the end).\n",
    "3. The Q-table entry at [0, 0] will be updated to (1 - alpha) * q_table[0, 0] + alpha * (reward + max_reward_at_next_state) := (1 - alpha) * <b>0</b> + alpha * (<b>0</b> + <b>0</b>) = <b>0</b>. Therefore, it stays the same. \n",
    "4. Now the agent can either stay at state (0) or go to state (4) (one step down). In either way, the process practically starts again at 1.\n",
    "\n",
    "### So the Q-table never gets updated and remains in its inital state. This can be seen during the training above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 2</h2>\n",
    "Very likely your training in Exercise 1 was not successful. Try to add exploration to your algorithm (you might have to write a new function):\n",
    "<li><code>I</code> $\\rightarrow$ Throw a random uniform number between 0 and 1. \n",
    "<li><code>II</code> $\\rightarrow$ If the number is smaller than 0.1, sample a random action.\n",
    "<li><code>III</code> $\\rightarrow$ Choose your action as usual.   \n",
    "    \n",
    "- Apply the modified $Q$-learning again to solve the environment.\n",
    "- Test the learned policy and animate one (or more) exemplary episode.\n",
    "- What do you observe? Does the agent learn now?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_q_learning_exploration(environment: lake_environment, alpha: float = 0.1):\n",
    "    \"\"\"\n",
    "    Solve lake_environment by applying Q learning and exploration.\n",
    "    \"\"\"\n",
    "    for i in range(1, 10001):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # <I>: Throw a random uniform number between 0 and 1.\n",
    "            random_number = np.random.uniform()\n",
    "            \n",
    "            # <II>: If the number is smaller than 0.1, sample a random action.\n",
    "            if random_number < 0.1:\n",
    "                action = environment.action_space.sample()\n",
    "                \n",
    "            # <III>: Else: Choose your action as usual.\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "                \n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episode: {i}\")\n",
    "                print(q_table)\n",
    "\n",
    "    print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "[[0.78764564 0.73993716 0.74464231 0.71572907]\n",
      " [0.46824384 0.4432702  0.39433624 0.65194753]\n",
      " [0.5316228  0.44112318 0.43590215 0.47005476]\n",
      " [0.22563011 0.08538446 0.09458473 0.13342553]\n",
      " [0.78835366 0.383867   0.3947618  0.52974961]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.42253322 0.19569666 0.2605268  0.19158155]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40168451 0.49888638 0.53086618 0.78894771]\n",
      " [0.52914774 0.7768856  0.48894954 0.52660474]\n",
      " [0.73318008 0.55043344 0.50000362 0.31364192]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.58640306 0.46298261 0.87415881 0.57793201]\n",
      " [0.84360624 0.94469809 0.86407091 0.88331754]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 9.08 s, sys: 1.4 s, total: 10.5 s\n",
      "Wall time: 8.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "apply_q_learning_exploration(lake_environment, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 57.08\n",
      "Average dives per episode: 0.31\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_dives = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "captured_frames_exploration = [[] for _ in range(episodes)]\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # reset variables\n",
    "    epochs, dives = 0, 0\n",
    "    state = lake_environment.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        epochs += 1\n",
    "        \n",
    "        # take best step with regard to the Q-table\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = lake_environment.step(action)\n",
    "        \n",
    "        # reset done if done = True because dive was taken\n",
    "        if done and lake_environment.s != 15:\n",
    "            dives += 1\n",
    "            lake_environment.reset()\n",
    "            done = False\n",
    "            \n",
    "        captured_frames_exploration[episode].append({\n",
    "            r'frame': lake_environment.render(mode=r'ansi'),\n",
    "            r'state': state,\n",
    "            r'action': action,\n",
    "            r'reward': reward\n",
    "        })\n",
    "        \n",
    "        # safety switch; abort if number of epochs exceeds the number of steps it takes the random search method to reach the goal\n",
    "        if epochs == 260:\n",
    "            break\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_dives += dives\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average dives per episode: {total_dives / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Step No.: 31\n",
      "State ID: 15\n",
      "Action ID: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames_exploration[4], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Step No.: 62\n",
      "State ID: 15\n",
      "Action ID: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames_exploration[40], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Step No.: 36\n",
      "State ID: 15\n",
      "Action ID: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames_exploration[99], verbose=True, delay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "Now, the Q-table gets updated in the learning process and the agent actually learns. Therefore, he nearly never takes unvoluntarily a dive. Moreover, the average steps per episode are far less than when using the random search algorithm from U8.\n",
    "\n",
    "Because of the added exploration, the agent doesn't chose the same action (0) every time. Therefore, eventually at some point the agent reaches the goal and gets an reward = 1. This results in the fact that the Q-table can be updated and isn't 0 anymore at some position. Now a chain reaction starts: Because not every Q-table entry is 0 anymore, in upcoming episodes another Q-table entry can be updated, because next_max is 1 at some point, etc. The agent is learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hands On AI 2",
   "language": "python",
   "name": "handsonai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
