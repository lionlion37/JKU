{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <p style=\"text-align: left;\">Name</p>               | Matr.Nr. | <p style=\"text-align: right;\">Date</p> |\n",
    "| --------------------------------------------------- | -------- | ------------------------------------- |\n",
    "| <p style=\"text-align: left\">Lion DUNGL</p> | 01553060 | 29.05.2020                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 7 -- Introduction to Natural Language Processing II </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors</b>: Rekabsaz, Brandstetter <br>\n",
    "<b>Date</b>: 11-05-2020\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0\n",
    "\n",
    "- Import the same modules as discussed in the lecture notebook.\n",
    "- Check if your model versions are correct.\n",
    "- Use your GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import u7_utils as u7\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.7 (✓)\n",
      "Installed numpy version: 1.18.1 (✓)\n",
      "Installed matplotlib version: 3.1.3 (✓)\n",
      "Installed PyTorch version: 1.5.0 (✓)\n"
     ]
    }
   ],
   "source": [
    "u7.check_module_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(208,90,80)\">ABOUT THIS NOTEBOOK</h1>\n",
    "<span style=\"color:rgb(208,90,80)\">In this notebook you should solve a small task on your one. <br><br> The goal is to train an LSTM network with a different number of hidden cells on the Penn Treebank dataset. You should decide on the validation dataset which model works best and then try it on the test dataset. This is a first example of a hyperparameter search. <br> We only evaluate how you build this hyperparameter search.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Defining hyper-parameters</h3>\n",
    "In contrast to the lecture notebook we do not set the parameter <i> nhid </i>. This is the hyperparameter which we will later use for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'resources/penn/'\n",
    "emsize = 200 # size of word embeddings\n",
    "lr = 20 # initial learning rate\n",
    "clipping = 0.25 # gradient clipping\n",
    "epochs = 3 # upper epoch limit\n",
    "train_batch_size = 10 # batch size for training\n",
    "eval_batch_size = 5 # batch size for elidation/test\n",
    "max_seq_len = 35 # sequence length\n",
    "seed = 1111 # random seed to facilitate reproducability\n",
    "print_interval = 1000 # report interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f11a1fbd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & dictionary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary 10001\n",
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n"
     ]
    }
   ],
   "source": [
    "train_corpus = u7.Corpus(os.path.join(data_path, 'train.txt'))\n",
    "valid_corpus = u7.Corpus(os.path.join(data_path, 'valid.txt'))\n",
    "test_corpus = u7.Corpus(os.path.join(data_path, 'test.txt'))\n",
    "\n",
    "dictionary = u7.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print (f'Number of tokens in dictionary {ntokens}')\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print (f'Train data: number of tokens {len(train_data)}')\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print (f'Validation data: number of tokens {len(valid_data)}')\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print (f'Test data: number of tokens {len(test_data)}')\n",
    "\n",
    "with open('dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batchified data shape: torch.Size([92958, 10])\n",
      "Validation batchified data shape: torch.Size([14752, 5])\n",
      "Test batchified data shape: torch.Size([16486, 5])\n"
     ]
    }
   ],
   "source": [
    "train_data_batches = u7.batchify(train_data, train_batch_size, device)\n",
    "print (f'Train batchified data shape: {train_data_batches.shape}')\n",
    "\n",
    "val_data_batches = u7.batchify(valid_data, eval_batch_size, device)\n",
    "print (f'Validation batchified data shape: {val_data_batches.shape}')\n",
    "\n",
    "test_data_batches = u7.batchify(test_data, eval_batch_size, device)\n",
    "print (f'Test batchified data shape: {test_data_batches.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training</h3>\n",
    "Nothing to do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, dictionary: u7.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, \n",
    "          train_data_batches, optimizer: torch.optim.Optimizer,\n",
    "          criterion: torch.nn, clipping: int, learning_rate: int,\n",
    "          print_interval: int, epoch: int):\n",
    "    \"\"\"\n",
    "    Function to train the model. \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = model.init_hidden(train_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = u7.get_batch(train_data_batches, i, max_seq_len)\n",
    "\n",
    "        # forward pass\n",
    "        model.zero_grad()\n",
    "        start_hidden = u7.repackage_hidden(start_hidden)\n",
    "        output, last_hidden = model(data, start_hidden)\n",
    "\n",
    "        # loss computation & backward pass\n",
    "        output = output.view(-1, ntokens)\n",
    "        loss = criterion(output, targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        start_hidden = last_hidden\n",
    "        # clipping gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % print_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch {epoch :3d} | {batch :5d} /{int(len(train_data_batches)/max_seq_len) :5d} batches ' \n",
    "                  f'| lr {learning_rate :02.2f} | ms/batch {elapsed * 1000 / print_interval :5.2f} |'\n",
    "                  f' loss {cur_loss :5.2f} | perplexity {math.exp(cur_loss) :8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid):\n",
    "        super(LM_LSTMModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(1, bsz, self.nhid),\n",
    "                weight.new_zeros(1, bsz, self.nhid))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        return F.log_softmax(decoded, dim=-1), last_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "- Train the model for three epochs and validate after each epoch. Repeat this procedure with different number of LSTM cells (the <i> nhid </i> parameter in the lecture notebook). Save the best models for the different runs.\n",
    "- What is the best model? You can use the suggested parameter values but you can try different values too if wanted. Please note that for larger number of LSTM cells the training might be pretty time-consuming.\n",
    "- Load the best model and evaluate it on the test dataset.\n",
    "- NOTA BENE: use the Adam optimizer to get better performance <code> optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)</code>, instead of SGD as done in the lecture (you can check for it in earlier notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid = [8, 16, 32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "1 / 7: Training with 8 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   0 |  1000 / 2655 batches | lr 20.00 | ms/batch  9.65 | loss  6.34 | perplexity   565.14\n",
      "| epoch   0 |  2000 / 2655 batches | lr 20.00 | ms/batch  9.57 | loss  5.92 | perplexity   371.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 26.35s| valid loss  5.83 | valid perplexity   341.97\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liondungl/.programs/miniconda3/envs/HandsOnAI2Venv/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LM_LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000 / 2655 batches | lr 20.00 | ms/batch  9.64 | loss  5.78 | perplexity   322.68\n",
      "| epoch   1 |  2000 / 2655 batches | lr 20.00 | ms/batch 10.27 | loss  5.73 | perplexity   306.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 27.58s| valid loss  5.73 | valid perplexity   308.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000 / 2655 batches | lr 20.00 | ms/batch  9.66 | loss  5.69 | perplexity   296.75\n",
      "| epoch   2 |  2000 / 2655 batches | lr 20.00 | ms/batch  9.53 | loss  5.67 | perplexity   289.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 26.18s| valid loss  5.69 | valid perplexity   296.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Done training after 80.17s !\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "2 / 7: Training with 16 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-843ccb8b6167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-48c8a9324017>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dictionary, max_seq_len, train_batch_size, train_data_batches, optimizer, criterion, clipping, learning_rate, print_interval, epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# loss computation & backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overall_start_time = time.time()\n",
    "for n, cells in enumerate(nhid):\n",
    "    print('-' * 89)\n",
    "    print(f'{n+1} / {len(nhid)}: Training with {cells} LSTM cells')\n",
    "    print('-' * 89)\n",
    "    print('-' * 89)\n",
    "        \n",
    "    save_path = os.path.join('models', 'nihd'+str(cells))\n",
    "    \n",
    "    model = LM_LSTMModel(ntokens, emsize, cells).to(device)\n",
    "\n",
    "    best_val_loss = None\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "    \n",
    "    cell_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, dictionary, max_seq_len, train_batch_size, train_data_batches, optimizer, criterion, clipping, lr, print_interval, epoch)\n",
    "        val_loss = u7.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_batches, criterion)\n",
    "        \n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch :3d} | time: {time.time() - epoch_start_time :5.2f}s' \n",
    "              f'| valid loss {val_loss :5.2f} | valid perplexity {math.exp(val_loss):8.2f}')\n",
    "        print('-' * 89)\n",
    "        \n",
    "        # saving best model\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr /= 4.0\n",
    "    else:\n",
    "        print('-' * 89)\n",
    "        print(f'Done training after {time.time() - cell_start_time :5.2f}s !')\n",
    "        print('-' * 89)\n",
    "else:\n",
    "    print('-' * 89)\n",
    "    print('-' * 89)\n",
    "    print('-' * 89)\n",
    "    print(f'Done after {time.time() - overall_start_time :5.2f}s !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "It seems like the more LSTM cells we're using, the better the coresponding model performs. But one can also see that the decrease of 'valid loss' and 'valid perplexity' gets smaller and smaller the more cells the model has. <b>The best model seems to be the one with 512 LSTM cells.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "1 / 7: Testing model with 8 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.65 | valid perplexity   284.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "2 / 7: Testing model with 16 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.45 | valid perplexity   232.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "3 / 7: Testing model with 32 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.29 | valid perplexity   197.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "4 / 7: Testing model with 64 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.16 | valid perplexity   174.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "5 / 7: Testing model with 128 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.07 | valid perplexity   158.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "6 / 7: Testing model with 256 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.03 | valid perplexity   153.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "-----------------------------------------------------------------------------------------\n",
      "7 / 7: Testing model with 512 LSTM cells\n",
      "-----------------------------------------------------------------------------------------\n",
      "| valid loss  5.01 | valid perplexity   149.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "                                                                                         \n",
      "!! The best performing model is the one with 512 LSTM cells and a loss on the validation set of 5.010598429501497 !!\n"
     ]
    }
   ],
   "source": [
    "best_num_cells = list()\n",
    "\n",
    "for n, cells in enumerate(nhid):\n",
    "    print('-' * 89)\n",
    "    print(f'{n+1} / {len(nhid)}: Testing model with {cells} LSTM cells')\n",
    "    print('-' * 89)\n",
    "    \n",
    "    model_path = os.path.join('models', 'nihd'+str(cells))\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    \n",
    "    val_loss = u7.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_batches, criterion)\n",
    "        \n",
    "    print(f'| valid loss {val_loss :5.2f} | valid perplexity {math.exp(val_loss):8.2f}')\n",
    "    print('-' * 89)\n",
    "    print(' ' * 89)\n",
    "    \n",
    "    if n == 0:\n",
    "        best_num_cells = [cells, val_loss]\n",
    "    else:\n",
    "        if val_loss < best_num_cells[1]:\n",
    "            best_num_cells = [cells, val_loss]\n",
    "else:\n",
    "    print(f'!! The best performing model is the one with {best_num_cells[0]} LSTM cells and a loss on the validation set of {best_num_cells[1]} !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  4.96 | test perplexity 142.69\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('models', 'nihd512'), 'rb') as f:\n",
    "    best_model = torch.load(f)\n",
    "    \n",
    "test_loss = u7.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_batches, criterion)\n",
    "\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss :5.2f} | test perplexity {math.exp(test_loss) :5.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "- Count the parameters of the best model. How many parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LSTM model with 512 cells has 8592985 parameters\n"
     ]
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in best_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The LSTM model with 512 cells has {n_params} parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hands On AI 2",
   "language": "python",
   "name": "handsonai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
