{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51eb9443",
   "metadata": {
    "cell_id": "ca70db1d-3eff-4494-8ef0-469a93cda65a",
    "deepnote_cell_height": 512.4000244140625,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2022*\n",
    "# Exercise 4: Evaluation\n",
    "\n",
    "In this exercise we'll have a closer look at two very different RecSys evaluation metrics and use them to compare the three algorithms we implemented so far to each other. Please consult the lecture slides and the presentation from UE Session 4 for a recap.\n",
    "\n",
    "The assignment submission deadline is 15.05.2022 23:59.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD22_ex03_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD22_ex03_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, as before, you are reqired to write a number of functions. Only implemented functions are graded. Insert your implementations into the templates provided. Please don't change the templates even if they are not pretty. Don't forget to test your implementation for correctness and efficiency.\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335978fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:34.422114Z",
     "start_time": "2022-06-21T18:56:33.505255Z"
    },
    "cell_id": "00001-f285fcf5-986d-4a87-9502-1957e782e29b",
    "deepnote_cell_height": 329.3999938964844,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1328,
    "execution_start": 1652977527294,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "801d7cd3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, pairwise_distances\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random as rnd\n",
    "import dill as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f061ba",
   "metadata": {
    "cell_id": "00002-076bae81-b6df-4d03-bfd4-a265d07a8c3b",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <font color='red'>TASK 1/2</font>: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dbbcf",
   "metadata": {
    "cell_id": "00003-40eefbd9-397b-4784-b1f6-f5d62ac15152",
    "deepnote_cell_height": 52.399993896484375,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement DCG, nDCG and Average Artist entropy in the corresponding templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f19f99",
   "metadata": {
    "cell_id": "00004-0ee02a78-bf7e-4097-bac3-e185ed01ce9e",
    "deepnote_cell_height": 490.3999938964844,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DCG Score\n",
    "Implement DCG following the input/output convention:\n",
    "#### Input:\n",
    "* prediction - (not an interaction matrix!) numpy array with recommendations. Row index corresponds to User_id, column index corresponds to the rank of the item mentioned in the sell. Every cell (i,j) contains **item id** recommended to the user (i) on the position (j) in the list. For example:\n",
    "\n",
    "The following predictions structure [[12, 7, 99], [0, 97, 6]] means that the user with id==1 (second row) got recommended item **0** on the top of the list, item **97** on the second place and item **6** on the third place.\n",
    "\n",
    "* test_interaction_matrix - (plain interaction matrix format as before!) interaction matrix constructed from interactions held out as a test set, rows - users, columns - items, cells - 0 or 1\n",
    "\n",
    "* topK - integer - top \"how many\" to consider for the evaluation. By default top 10 items are to be considered\n",
    "\n",
    "#### Output:\n",
    "* DCG score\n",
    "\n",
    "Don't forget, DCG is calculated for every user separately and then the average is returned.\n",
    "\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted! Users without interactions in the test set shouldn't contribute to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b1b471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:36.457022Z",
     "start_time": "2022-06-21T18:56:36.443793Z"
    },
    "cell_id": "00005-476853da-3eda-4f53-a8f1-f8f8963f447b",
    "deepnote_cell_height": 567,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977528630,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "add5336f"
   },
   "outputs": [],
   "source": [
    "def get_dcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK = 10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "    \n",
    "    returns - float - mean dcg score over all user.\n",
    "    \"\"\"    \n",
    "    score = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    dcg_scores = []\n",
    "\n",
    "    for user_id, pred in enumerate(predictions):\n",
    "\n",
    "        if sum(test_interaction_matrix[user_id]) == 0 or np.all(pred == -1):  # ignore users w/o interaction in test matrix or w/o predictions \n",
    "                continue\n",
    "\n",
    "        current_dcg = 0\n",
    "\n",
    "        for k, item_id in enumerate(pred[:topK]):\n",
    "            current_dcg += test_interaction_matrix[user_id, item_id] / np.log2(k + 2)\n",
    "\n",
    "        dcg_scores.append(current_dcg)\n",
    "\n",
    "    score = np.mean(dcg_scores)\n",
    "            \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1713e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:38.919857Z",
     "start_time": "2022-06-21T18:56:38.916938Z"
    },
    "cell_id": "00006-57e3b499-cc35-4d7b-991c-f2b6159ae6a0",
    "deepnote_cell_height": 171,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1652977528632,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "1ba4466c"
   },
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "dcg_score = get_dcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(dcg_score, 1), \"1 expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9190d",
   "metadata": {
    "cell_id": "00007-27f6a41b-afb6-46a2-a753-6f0b37d81b80",
    "deepnote_cell_height": 242.01666259765625,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Can DCG score be higher than 1?\n",
    "Yes. Assume we have 3 topK items and there was an interaction with each one of them (highest possible gain $\\rightarrow$ highest DCG). Then $\\text{DCG}=1+\\frac{1}{log_2(3)}+\\frac{1}{log_2(4)}=2.1309... > 1$\n",
    "* Can the average DCG score be higher than 1?\n",
    "Yes. If we again assume to have 3 topK items and there was an interaction of every user with each recommended item, the individual DCGs would be $2.1309...$ (see above) which in this case would also be the average.\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edd396",
   "metadata": {
    "cell_id": "00008-0458efe4-174d-4711-ae4b-9880f2557fa3",
    "deepnote_cell_height": 159.1999969482422,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nDCG Score\n",
    "\n",
    "Following the same parameter convention as for DCG implement nDCG metric.\n",
    "\n",
    "<font color='red'>**Attention!**</font> Remember that ideal DCG is calculated separetely for each user and depends on the number of tracks held out for them as a Test set! Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4631f59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:40.074403Z",
     "start_time": "2022-06-21T18:56:40.058907Z"
    },
    "cell_id": "00009-cf5a3b33-b5ca-4dcf-85c7-84f2db1f4fc1",
    "deepnote_cell_height": 819,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977528651,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "a87100fe"
   },
   "outputs": [],
   "source": [
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK = 10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "    topK - int - topK recommendations should be evaluated.\n",
    "    \n",
    "    returns - average ndcg score over all users.\n",
    "    \"\"\"    \n",
    "    score = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "        \n",
    "    scores = []\n",
    "\n",
    "    for user_id, pred in enumerate(predictions):\n",
    "        \n",
    "        if sum(test_interaction_matrix[user_id]) == 0 or np.all(pred == -1):  # ignore users w/o interaction in test matrix or w/o predictions \n",
    "            continue\n",
    "\n",
    "        # DCG\n",
    "        current_dcg = 0\n",
    "\n",
    "        for k, item_id in enumerate(pred[:topK]):\n",
    "            current_dcg += test_interaction_matrix[user_id, item_id] / np.log2(k + 2)\n",
    "\n",
    "        # iDCG\n",
    "        total_interactions = sum(test_interaction_matrix[user_id])\n",
    "        icg = total_interactions  # ideal cumulative gain\n",
    "        if total_interactions >= topK:\n",
    "            icg = topK\n",
    "\n",
    "        current_idcg = 0\n",
    "        for k in range(int(icg)):\n",
    "            current_idcg += 1 / np.log2(k + 2)\n",
    "\n",
    "        # nDCG\n",
    "        scores.append(current_dcg / current_idcg)\n",
    "    \n",
    "    \n",
    "    score = np.mean(scores)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2372b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:42.344617Z",
     "start_time": "2022-06-21T18:56:42.338859Z"
    },
    "cell_id": "00010-235ca1a1-3aed-4804-ac29-3d3305ac9c65",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     309
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977528655,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "417c32be"
   },
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0], [1, 2, 3, 0], [-1, -1, -1, -1]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "ndcg_score = get_ndcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "assert np.isclose(ndcg_score, 1), \"ndcg score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e53db",
   "metadata": {
    "cell_id": "00011-c76176b5-d5c1-4c59-992f-49ca14422ca1",
    "deepnote_cell_height": 92.30000305175781,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Can nDCG score be higher than 1?\n",
    "No. In the optimal case $\\text{DCG} = \\text{iDCG}$, which means that that $\\text{nDCG}=1$, since this score describes the relative share of the $\\text{DCG}$ with respect to the ideal $\\text{iDCG}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c4388",
   "metadata": {
    "cell_id": "00012-0e38339b-706a-4cad-b0ac-9adee312a3b4",
    "deepnote_cell_height": 487.3999938964844,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Average Artist Entropy\n",
    "\n",
    "Calculate the metric of Diversity as the Average Artist entropy (see UE slides).\n",
    "#### Parameters:\n",
    "* predictions - as above for DCG and nDCG;\n",
    "* item_df - dataframe containing 'artist' and 'track' columns, index - track id (use corresponding data file)\n",
    "* topK - depth of the list to be evaluated, as before\n",
    "\n",
    "#### Result:\n",
    "Average Artist Entropy over users\n",
    "\n",
    "Recap, main points:\n",
    "* First calculate diversity for each user, then return the mean over users\n",
    "* For every user build distribution of recommended tracks over artists (within topK). This distribution cannot have more than topK bins! Dont forget to turn it into probability distribution dividing it by topK\n",
    "* Use the formula from the UE slides for the per-user entropy\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a62911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:43.854452Z",
     "start_time": "2022-06-21T18:56:43.850905Z"
    },
    "cell_id": "00013-34dee0a5-93de-498b-83f7-e443fc66d75e",
    "deepnote_cell_height": 585,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652977528684,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "51665d7c"
   },
   "outputs": [],
   "source": [
    "def get_average_entropy_score(predictions: np.ndarray, item_df: pd.DataFrame, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    item_df - pd.DataFrame - information about each song with columns 'artist' and 'track'.\n",
    "    \n",
    "    returns - float - average entropy score of the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    score = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    diversities = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        if np.all(pred == -1):  # no recommendations\n",
    "            continue\n",
    "\n",
    "        current_top = topK\n",
    "        if -1 in pred:\n",
    "            current_top = list(pred).index(-1) + 1\n",
    "\n",
    "        artists = item_df['artist'][pred[:current_top]]\n",
    "        artist_counts = np.unique(artists, return_counts=True)[1]\n",
    "        diversities.append((-np.sum((artist_counts / current_top) * np.log2(artist_counts / current_top)) / np.log2(current_top)))  # entropy / max_entropy\n",
    "\n",
    "    score = np.mean(diversities)\n",
    "                \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916052bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:45.116737Z",
     "start_time": "2022-06-21T18:56:45.101296Z"
    },
    "cell_id": "00014-fc82d4bd-2b83-4793-a5d5-ccf5a8230555",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652977528685,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "242abcd7"
   },
   "outputs": [],
   "source": [
    "item_df = pd.DataFrame({'artist': ['A1', 'A1', 'A1', 'A1', 'A2', 'A3', 'A4']})\n",
    "predictions = np.array([[0, 1, 2, 3], [6, 5, 4, 3], [-1, -1, -1, -1]])\n",
    "\n",
    "avg_entr_score = get_average_entropy_score(predictions, item_df, topK=4)\n",
    "assert np.isclose(avg_entr_score, 0.5), \"average entropy score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833a672",
   "metadata": {
    "cell_id": "00015-ab2bffd4-a5e4-4fcd-ade6-c099750d6931",
    "deepnote_cell_height": 263.6000061035156,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <font color='red'>TASK 2/2</font>: Evaluation\n",
    "Use provided rec.py (see imports below) to build a simple evaluation framework. It should be able to evaluate POP, ItemKNN (CF) and SVD.\n",
    "\n",
    "In the end for each algorithm you should be able to obtain results formatted as follows:\n",
    "\n",
    "```\n",
    "{'m': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'f': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'all': {'ndcg': <>, 'average_entropy': <>}}\n",
    "```\n",
    " \n",
    "Every metric calculated for three groupls of test users: only for female, only for male and for all users together.\n",
    "Every value should be an average, calculated over two different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8179a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:48.873495Z",
     "start_time": "2022-06-21T18:56:48.865465Z"
    },
    "cell_id": "00016-d8b27c10-5ee7-45a9-85d1-d555cb728847",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652977528687,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "f068620c"
   },
   "outputs": [],
   "source": [
    "from rec import svd_decompose, svd_recommend_to_list\n",
    "from rec import inter_matr_binary, split_interactions\n",
    "from rec import recTopK\n",
    "from rec import recTopKPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a928b4a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:49.452557Z",
     "start_time": "2022-06-21T18:56:49.448179Z"
    },
    "cell_id": "00017-9d5988ec-d2a7-4cd1-8914-0fdb136c972a",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 24,
    "execution_start": 1652977528691,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "33ef7e47"
   },
   "outputs": [],
   "source": [
    "path = 'ex05/MRS_Challenge_2022_data/'\n",
    "\n",
    "usr_path = path + 'MRSC_2022_demo.txt'\n",
    "itm_path = path + 'MRSC_2022_tracks.txt'\n",
    "inter_path = path + 'MRSC_2022_inter.txt'\n",
    "\n",
    "challenge = path + 'MRSC_2022_target_users.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0382492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:50.965440Z",
     "start_time": "2022-06-21T18:56:50.949119Z"
    }
   },
   "outputs": [],
   "source": [
    "target_users = np.array(pd.read_csv(challenge, sep='\\t')['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c524",
   "metadata": {
    "cell_id": "00018-9ee6cd3b-1c0f-440b-80de-96fd486747a2",
    "deepnote_cell_height": 159.1999969482422,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (A) Universal Launcher\n",
    "Receives a config as described, train and test interaction matrices.\n",
    "\n",
    "Returns a matrix of size (total number of users) x (topK). cells - recommended item ids, sorted according to the score. Fill the cells corresponding to users with no interactions in the test set with (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2788c",
   "metadata": {
    "cell_id": "00020-b7c959c1-1db1-45a6-bf96-51530d4e88c8",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single set evaluation (already implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b6bc7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:52.187452Z",
     "start_time": "2022-06-21T18:56:52.178628Z"
    },
    "cell_id": "00021-3410b138-ec7a-48ec-9e6f-12e9a8aec315",
    "deepnote_cell_height": 459,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1652977528720,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "57a9c040"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions: np.ndarray, test_interaction_matrix: np.ndarray, \n",
    "                         item_df: pd.DataFrame, topK=10) -> dict:\n",
    "    \"\"\"\n",
    "    This function returns a dictinary with all scores of predictions.\n",
    "    \n",
    "    predictions - np.ndarray - predictions of the algorithm over all users.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix over all users and items.\n",
    "    item_df - pd.DataFrame - information about each item with columns: 'artist', 'track'\n",
    "    topK - int - topK prediction should be evaluated\n",
    "    \n",
    "    returns - dict - calculated metric scores, contains keys \"ndcg\" and \"average_entropy\".\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    ndcg = get_ndcg_score(predictions, test_interaction_matrix, topK)\n",
    "    metrics['ndcg'] = ndcg\n",
    "    \n",
    "    average_entropy = get_average_entropy_score(predictions, item_df, topK)\n",
    "    metrics['average_entropy'] = average_entropy\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8b1af",
   "metadata": {
    "cell_id": "00022-e65007c0-00d4-431f-bffe-04deefd469a4",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (B) User-group evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff3a7f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:53.212752Z",
     "start_time": "2022-06-21T18:56:53.201691Z"
    },
    "cell_id": "00023-7dcdefb7-6eb4-4265-9eac-2eda7feb763c",
    "deepnote_cell_height": 567,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15,
    "execution_start": 1652977528730,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "20198af2"
   },
   "outputs": [],
   "source": [
    "def evaluate_gender(predictions: np.ndarray, test_interaction_matrix: np.ndarray, user_df: pd.DataFrame, \n",
    "                    item_df: pd.DataFrame, num_users=500, topK=10) -> dict:\n",
    "    \"\"\"\n",
    "    This function will evaluate certain predictions for each gender individually and return a dictionary\n",
    "    following the structure:\n",
    "    \n",
    "    {'gender_key': {'metric_key': metric_score}}\n",
    "    \n",
    "    predictions - np.ndarray - predictions of the algorithm over all users.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix over all users and items.\n",
    "    user_df - pd.DataFrame - information about each user with columns: location', 'age', 'gender', 'date'\n",
    "    item_df - pd.DataFrame - information about each item with columns: 'artist', 'track'\n",
    "    topK - int - topK prediction should be evaluated\n",
    "    \n",
    "    returns - dict - calculated metric scores for each gender.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    \n",
    "    for gender in ['m', 'f']:\n",
    "        gender_ids = np.where(user_df['gender'] == gender)\n",
    "        metrics[gender] = evaluate_predictions(predictions[gender_ids], test_interaction_matrix[gender_ids], item_df, topK)\n",
    "        \n",
    "    metrics['all'] = evaluate_predictions(predictions, test_interaction_matrix, item_df, topK)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0079",
   "metadata": {
    "cell_id": "00024-daa41acb-cb08-40cd-ae1d-6edf6c88863b",
    "deepnote_cell_height": 219.1999969482422,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (C) Main evaluation function\n",
    "Interprets the config and returns evaluation report for a single algorithm:\n",
    "```\n",
    "{'m': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'f': {'ndcg': <>, 'average_entropy': <>},\n",
    " 'all': {'ndcg': <>, 'average_entropy': <>}}\n",
    "```\n",
    "\n",
    "Please pay attention to how splits are created and saved into the corresponding variables (Split Data section below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708f448e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:54.461964Z",
     "start_time": "2022-06-21T18:56:54.445757Z"
    },
    "cell_id": "00025-c4d58cc5-1902-4e81-9b1f-b77ec184d182",
    "deepnote_cell_height": 945,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652977528767,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "b779580f"
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithm(config) -> dict:\n",
    "    \"\"\"\n",
    "    This function will evaluate a certain algorithm defined with the parameters in config by:\n",
    "    - going over all test and train files\n",
    "    - generating the recommendations for each data split\n",
    "    - calling evaluate gender to get the metrics for each recommendation for each data split\n",
    "    \n",
    "    Then the average score for each gender and metric should be calculated over all data splits and\n",
    "    a dictionary should be returned following the structure:\n",
    "    {'gender_key': {'metric_key': avg_metric_score}}\n",
    "    \n",
    "    config - dict - configuration of this evaluation following the structure:\n",
    "    \n",
    "    config = {\n",
    "        \"algorithm\": str - one of ['SVD', 'CF', 'TopPop']\n",
    "        \"inter_train_file_paths\": str - array of inter train file paths (1 per split),\n",
    "        \"inter_test_file_paths\": str - array of inter test file paths (1 per split),\n",
    "        \"user_file_path\": str - usr_path,\n",
    "        \"item_file_path\": str - itm_path,\n",
    "        \"top_k\": int - number of recommendations to evaluate\n",
    "        \"n\": int - used for CF.\n",
    "        \"f\": int - length of hidden representations for SVD\n",
    "    }\n",
    "    \n",
    "    returns - dict - average score of each metric for each gender over all data splits.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    split_metrics = []\n",
    "    df_users = pd.read_csv(config['user_file_path'], sep='\\t', header=None, names=['location','age','gender', 'date'])\n",
    "    df_items = pd.read_csv(config['item_file_path'], sep='\\t', header=None, names=['artist','track'])\n",
    "\n",
    "    for train_split_path, test_split_path in zip(config['inter_train_file_paths'], config['inter_test_file_paths']):\n",
    "        train_split = inter_matr_binary(config['user_file_path'], config['item_file_path'], train_split_path)\n",
    "        test_split = inter_matr_binary(config['user_file_path'], config['item_file_path'], test_split_path)\n",
    "\n",
    "        current_recs = get_recommendations_for_algorithm(config, train_split, test_split)\n",
    "        split_metrics.append(evaluate_gender(current_recs, test_split, df_users, df_items, config['top_k']))\n",
    "\n",
    "    for key in ['m', 'f', 'all']:\n",
    "        current_dict = {}\n",
    "        for score in ['ndcg', 'average_entropy']:\n",
    "            current_dict[score] = np.mean([curr_split[key][score] for curr_split in split_metrics])\n",
    "        metrics[key] = current_dict\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c270ba",
   "metadata": {
    "cell_id": "00026-db254db1-573b-4501-9000-9c336296755e",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Splitting Data (already implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c4bfb",
   "metadata": {
    "cell_id": "00027-a625a3b7-5c60-4325-aba3-917360418ff4",
    "deepnote_cell_height": 586.7999877929688,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3956,
    "execution_start": 1652977528769,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "79c9a654"
   },
   "outputs": [],
   "source": [
    "train_inter_files = []\n",
    "test_inter_files = []\n",
    "\n",
    "num_splits = 2\n",
    "p_i = 0.3\n",
    "p_u = 0.5\n",
    "\n",
    "user_file_path = None\n",
    "inter_file_path = None\n",
    "\n",
    "user_file_path = usr_path\n",
    "inter_file_path = inter_path\n",
    "\n",
    "for i in range(num_splits):\n",
    "    \n",
    "    split_interactions(inter_file=inter_file_path,\n",
    "                       user_file_path = user_file_path,\n",
    "                       p_u = p_u,\n",
    "                       p_i = p_i,\n",
    "                       res_test_file=\"inter_TEST_\" + str(i) + \".txt\",\n",
    "                       res_train_file=\"inter_TRAIN_\" + str(i) + \".txt\")\n",
    "    \n",
    "    train_inter_files.append(\"inter_TRAIN_\" + str(i) + \".txt\")\n",
    "    test_inter_files.append(\"inter_TEST_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17ccc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:57.604485Z",
     "start_time": "2022-06-21T18:56:57.596197Z"
    }
   },
   "outputs": [],
   "source": [
    "num_splits = 2\n",
    "p_i = 0.3\n",
    "p_u = 0.5\n",
    "train_inter_files = ['ex05/train_test_data/inter_TRAIN_0.txt', 'ex05/train_test_data/inter_TRAIN_1.txt']\n",
    "test_inter_files = ['ex05/train_test_data/inter_TEST_0.txt', 'ex05/train_test_data/inter_TEST_0.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5734176d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:57.886179Z",
     "start_time": "2022-06-21T18:56:57.878762Z"
    },
    "cell_id": "00028-f48b6df6-d100-4e3a-9679-54b1ebf4c66d",
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977532721,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "ec8a4b83"
   },
   "outputs": [],
   "source": [
    "assert len(train_inter_files) == num_splits, \"Number of Train files do not match the requirement\"\n",
    "assert len(test_inter_files) == num_splits, \"Number of Test files do not match the requirement\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee1a7b",
   "metadata": {
    "cell_id": "00029-114c53f8-4ccb-42b8-9ff5-eb2a9f3111be",
    "deepnote_cell_height": 100.39999389648438,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating Every Algorithm\n",
    "Make sure everything works. Try running evaluation with the three configs below.\n",
    "We expect KNN to outperform other algorithms on our small data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b587be53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T18:56:59.084791Z",
     "start_time": "2022-06-21T18:56:59.064941Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations_for_algorithm(config, inter_matrix_train, inter_matrix_test) -> np.ndarray:\n",
    "    \n",
    "    rec = None\n",
    "    \n",
    "    df_users = pd.read_csv(config['user_file_path'], sep='\\t', header=None, names=['location','age','gender', 'date'])\n",
    "    df_items = pd.read_csv(config['item_file_path'], sep='\\t', header=None, names=['artist','track'])\n",
    "    \n",
    "    rec = np.full((len(df_users), config['top_k']), -1)\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    \n",
    "    user_ids = np.where(np.array([np.all(user == 0) for user in inter_matrix_test]) == False)[0]  # get user_ids of users with at least 1 test-interaction\n",
    "\n",
    "    if config['algorithm'] == 'SVD':\n",
    "        U, V = svd_decompose(inter_matrix_train, f=config['f'])\n",
    "\n",
    "        # get ids of seen items\n",
    "        seen_item_ids = []\n",
    "        for interactions in inter_matrix_train[user_ids]:\n",
    "            seen_item_ids.append(list(np.where(interactions == 1)[0]))\n",
    "            \n",
    "        rec = svd_recommend_to_list(user_ids, seen_item_ids, U, V, config['top_k'])\n",
    "\n",
    "    elif config['algorithm'] == 'CF':\n",
    "        for user in user_ids:\n",
    "            current_recs = recTopK(inter_matrix_train, user, config['top_k'], config['n'])\n",
    "            rec[user] = current_recs[0]\n",
    "    \n",
    "    elif config['algorithm'] == 'TopPop':\n",
    "        for user in user_ids:\n",
    "            current_recs = recTopKPop(inter_matrix_train, user, config['top_k'])\n",
    "            rec[user] = current_recs\n",
    "            \n",
    "    elif config['algorithm'] == 'Neighbor':\n",
    "        print('Calculating distances...')\n",
    "        #dist = cosine_distances(inter_matrix_train.T, inter_matrix_train.T)\n",
    "        dist = pairwise_distances(inter_matrix_train.T, inter_matrix_train.T, metric=\"jaccard\", n_jobs=-1)\n",
    "        print('Done')\n",
    "    \n",
    "        for user in tqdm(user_ids):\n",
    "            interactions = inter_matrix_train[user]\n",
    "            unseen_item_ids = list(np.where(interactions == 0)[0])\n",
    "            seen_item_ids = list(np.where(interactions == 1)[0])\n",
    "            n_seen = len(seen_item_ids)\n",
    "            usr_dist = dist[:,unseen_item_ids][seen_item_ids]\n",
    "            #itm_dist = np.sort(usr_dist.T)[:,:config['n']].sum(axis=1) / config['n']\n",
    "            itm_dist = usr_dist.sum(axis=0)\n",
    "            ranking = np.argsort(itm_dist)\n",
    "            unseen_item_ids = np.array(unseen_item_ids)\n",
    "            rec[user] = unseen_item_ids[ranking[:config['top_k']]]\n",
    "    \n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e51660a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T19:15:50.162309Z",
     "start_time": "2022-06-21T19:12:57.482182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distances...\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1298/1298 [02:52<00:00,  7.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "path = 'ex05/MRS_Challenge_2022_data/'\n",
    "\n",
    "usr_path = path + 'MRSC_2022_demo.txt'\n",
    "itm_path = path + 'MRSC_2022_tracks.txt'\n",
    "inter_path = path + 'MRSC_2022_inter.txt'\n",
    "df_users = pd.read_csv(usr_path, sep='\\t', header=None, names=['location','age','gender', 'date'])\n",
    "df_items = pd.read_csv(itm_path, sep='\\t', header=None, names=['artist','track'])\n",
    "\n",
    "challenge = path + 'MRSC_2022_target_users.txt'\n",
    "target_users = np.array(pd.read_csv(challenge, sep='\\t')['1'])\n",
    "\n",
    "#inter_matrix = inter_matr_binary(usr_path, itm_path, inter_path)\n",
    "\n",
    "# rec = np.full((len(target_users), 15), -1)\n",
    "rec = []\n",
    "\n",
    "print('Calculating distances...')\n",
    "#dist = cosine_distances(inter_matrix_train.T, inter_matrix_train.T)\n",
    "#dist = pairwise_distances(inter_matrix.T, inter_matrix.T, metric=\"jaccard\", n_jobs=-1)\n",
    "print('Done')\n",
    "\n",
    "for n, user in enumerate(tqdm(target_users)):\n",
    "    interactions = inter_matrix[user]\n",
    "    unseen_item_ids = list(np.where(interactions == 0)[0])\n",
    "    seen_item_ids = list(np.where(interactions == 1)[0])\n",
    "\n",
    "    usr_dist = dist[:,unseen_item_ids][seen_item_ids]\n",
    "    #itm_dist = np.sort(usr_dist.T)[:,:config['n']].sum(axis=1) / config['n']\n",
    "    itm_dist = usr_dist.sum(axis=0)\n",
    "    ranking = np.argsort(itm_dist)\n",
    "    unseen_item_ids = np.array(unseen_item_ids)\n",
    "    usr_recs = unseen_item_ids[ranking[:15]]\n",
    "    rec.append([user, ','.join(usr_recs.astype(str))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c9626c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T19:22:21.611757Z",
     "start_time": "2022-06-21T19:22:21.606635Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('rec_k01553060_Dungl_Lion.tsv', 'w', newline='\\n') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    for r in rec:\n",
    "        writer.writerow([str(r[0]), r[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8587e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T19:26:25.113223Z",
     "start_time": "2022-06-21T19:26:25.080460Z"
    }
   },
   "outputs": [],
   "source": [
    "for u, r in rec:\n",
    "    u_r = np.array(r.split(','), dtype=int)\n",
    "    if 1 in inter_matrix[u][u_r]:\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5c53919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T20:24:31.822272Z",
     "start_time": "2022-06-21T19:50:01.158403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liondungl/.programs/miniconda3/envs/deeplearning/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6499/6499 [14:53<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liondungl/.programs/miniconda3/envs/deeplearning/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1776: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6499/6499 [14:42<00:00,  7.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'m': {'ndcg': 0.08865760783175179, 'average_entropy': 0.9995653822251053},\n",
       " 'f': {'ndcg': 0.08957222455779476, 'average_entropy': 0.999707501218788},\n",
       " 'all': {'ndcg': 0.0888314117487641, 'average_entropy': 0.9995923889887918}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Item KNN (CF)\n",
    "config = {\n",
    "    \"algorithm\": \"Neighbor\", # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 15, # number of recommendations.\n",
    "    \"n\": 5, # used for CF.\n",
    "    \"f\": 32, # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57256b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T10:22:16.091706Z",
     "start_time": "2022-06-21T10:22:16.087623Z"
    },
    "cell_id": "00030-99cf7fa4-44fc-4f98-9aab-b886d85c8747",
    "deepnote_cell_height": 391,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     60,
     60
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2682,
    "execution_start": 1652977532721,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "source_hash": "4bc548e1"
   },
   "outputs": [],
   "source": [
    "# Evaluate TopPop\n",
    "config = {\n",
    "    \"algorithm\": \"TopPop\", # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 10, # number of recommendations.\n",
    "    \"n\": 5, # used for CF.\n",
    "    \"f\": 32, # length of hidden representations\n",
    "}\n",
    "\n",
    "#scores = evaluate_algorithm(config)\n",
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9d982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T17:57:22.366099Z",
     "start_time": "2022-06-21T17:56:00.442624Z"
    },
    "cell_id": "00031-93679e20-cca2-4d9a-9eeb-e9c4639a18c7",
    "deepnote_cell_height": 391,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     60,
     60
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 263587,
    "execution_start": 1652977535406,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "6fa4cd18"
   },
   "outputs": [],
   "source": [
    "# Evaluate Item KNN (CF)\n",
    "config = {\n",
    "    \"algorithm\": \"Neighbor\", # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 15, # number of recommendations.\n",
    "    \"n\": 5, # used for CF.\n",
    "    \"f\": 32, # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f49e95",
   "metadata": {
    "cell_id": "00032-f3f1d583-f824-481e-9e40-7f91d1dfd659",
    "deepnote_cell_height": 391,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     60,
     60
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 94304,
    "execution_start": 1652977798991,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "source_hash": "8c1692cf"
   },
   "outputs": [],
   "source": [
    "# Evaluate SVD\n",
    "config = {\n",
    "    \"algorithm\": \"SVD\", # ['SVD', 'CF', 'TopPop']\n",
    "    \"inter_train_file_paths\": train_inter_files,\n",
    "    \"inter_test_file_paths\": test_inter_files,\n",
    "    \"user_file_path\": usr_path,\n",
    "    \"item_file_path\": itm_path,\n",
    "    \"top_k\": 10, # number of recommendations.\n",
    "    \"n\": 5, # used for CF.\n",
    "    \"f\": 256, # length of hidden representations\n",
    "}\n",
    "\n",
    "scores = evaluate_algorithm(config)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59375671",
   "metadata": {
    "cell_id": "00033-00dd6cdc-3c83-4048-8491-8405dee4dc0b",
    "deepnote_cell_height": 189,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977893291,
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "df61fc7f"
   },
   "outputs": [],
   "source": [
    "g_keys = ['m', 'f', 'all']\n",
    "m_keys = ['ndcg', 'average_entropy']\n",
    "\n",
    "assert all([k in g_keys for k in scores.keys()]), 'keys error'\n",
    "assert all([k in m_keys for k in scores['m'].keys()]), 'keys error'\n",
    "assert scores['all']['ndcg'] >= 0, \"metric score should be a number.\"\n",
    "assert scores['all']['average_entropy'] >= 0, \"metric score should be a number.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62556d70",
   "metadata": {
    "cell_id": "00034-e49d6ddd-c1ec-4976-bfc7-04362988f773",
    "deepnote_cell_height": 1126.800048828125,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions and Potential Future Work\n",
    "* Do all algorithms treat Female and Male users similarly? Why?\n",
    "No. For TopPop and SVD the nDCG is larger for Female users than for Male ones. For SVD it is the other way around. Those differences though are changing each time executing the whole notebook, so sometimes for example the Male-nDCG for SVD is larger then the Female-nDCG (possible reason being different random train-test splits). So in total, there is not a really significant difference between Male and Female. A reason for a hypothetical difference could be more users of one gender in the train set. Regarding Entropy there is no significant difference, since it is always very close to 1.\n",
    "* How would you try improve performance of all three algorithms?\n",
    "-- Increase size of dataset if possible<br>\n",
    "-- Online Testing to get a better estimate of the real values of the used metrics<br>\n",
    "-- Using more metrics<br>\n",
    "-- Increase model complexity until overfitting to find optimal model<br>\n",
    "* What other metrics would you consider to compare these recommender systems?\n",
    "-- Different diversities, e.g. w.r.t. genre<br>\n",
    "-- generally beyond-accuracy metrics (e.g. Novelty) to achieve better user satisfaction<br>\n",
    "-- Recall/Precision/F-measure; disadvantage: order of recommendations is not considered<br>\n",
    "-- Reciprocal Rank to take order of recommendations into account\n",
    "* What other user groups would you investigate?\n",
    "User groups w.r.t...<br>\n",
    "-- Age<br>\n",
    "-- Nationality<br>\n",
    "-- Income<br>\n",
    "-- Phone brand<br>\n",
    "-- Premium/Paying vs. Non-Premium/Non-Paying users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f290a9f",
   "metadata": {
    "cell_id": "00035-0324d241-8f2f-478f-9668-a8749912f959",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652977893292,
    "owner_user_id": "c6f73f6d-b02b-413f-a493-796763fa488b",
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "d1b4db5d"
   },
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a90b0940-3f10-40bd-811b-80cd611d38a6",
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
